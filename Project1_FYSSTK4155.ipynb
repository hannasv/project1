{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: fys-stk4155"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "from random import random, seed\n",
    "from imageio import imread\n",
    "from resample import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate random data and make a plot of using the Franke function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (utils.py, line 165)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"c:\\users\\nbpst2\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m2961\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-23541eb2ec0d>\"\u001b[1;36m, line \u001b[1;32m1\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    from utils import franke_function\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\nbpst2\\Documents\\GitHub\\project1\\utils.py\"\u001b[1;36m, line \u001b[1;32m165\u001b[0m\n\u001b[1;33m    <<<<<<< HEAD\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from utils import franke_function\n",
    "fig = plt.figure(figsize = (10,6))\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# Make data.\n",
    "x = np.arange(0, 1, 0.05)\n",
    "y = np.arange(0, 1, 0.05)\n",
    "x, y = np.meshgrid(x,y)\n",
    "\n",
    "z = franke_function(x, y)\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(x, y, z, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "# Customize the z axis.\n",
    "ax.set_zlim(-0.10, 1.40)\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure\n",
    "plt.contourf(x, y, z, cmap=\"RdBu_r\")\n",
    "plt.colorbar()\n",
    "plt.title(\"Contours of Franke's function\", fontsize=14)\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "# plt.savefig(\"results/figures/franke/franke.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the models\n",
    "\n",
    "## Select the models in terms of the parameters that minimize the MSE and the R2 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import our functions written for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms\n",
    "from model_selection0 import GridSearchNew #use Gridsearch.fit(x,y)\n",
    "from model_comparison0 import model_comparison0\n",
    "from utils import generateDesignmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a random sample to test our code with the Franke function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use this seed in order to raplicate the experiment with the same data\n",
    "np.random.seed(1000)  \n",
    "\n",
    "# Data\n",
    "x = np.random.rand(1000, )\n",
    "y = np.random.rand(1000, )\n",
    "z = franke_function(x, y) + np.random.normal(0, 0.1, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental setup\n",
    "\n",
    "## Define the models and the parameters that we want to compare.\n",
    "The model are Ordinary Least Squares (OLS), Ridge and Lasso.\n",
    "The parameter are []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"ols\": algorithms.OLS, \n",
    "    'ridge': algorithms.Ridge, \n",
    "    \"lasso\": algorithms.Lasso\n",
    "}\n",
    "param_grid = {\n",
    "    'ols': [0],\n",
    "    'ridge': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100, 1000],  \n",
    "    'lasso': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0100, 1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform experiment and collect results.\n",
    " \n",
    "The sample is splitted leaving 80% of the data for training the model and 20% for testing it.\n",
    "\n",
    "The polynomial order varies from 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mse_ols = []\n",
    "for p in np.arange(1,6,1):\n",
    "    X = generateDesignmatrix(p,x,y)\n",
    "    \n",
    "    results, z_pred_best = model_comparison0(\n",
    "    models, param_grid, X, z, split_size=0.2\n",
    "    )\n",
    "\n",
    "    # write loop instead of repeating code!!!\n",
    "    \n",
    "    # One figure for MSE and all pol. orders\n",
    "\n",
    "    # Subplot for ridge\n",
    "    plt.figure(1, figsize = (11,7))\n",
    "    plt.subplot(221)\n",
    "    # log x-axis\n",
    "    xlogr = np.log10(param_grid['ridge'])\n",
    "    plt.plot(xlogr, results[\"mse_test\"][\"ridge\"][0], label='p = %s' % p, linewidth=3.0) # plot ridge\n",
    "    ax = plt.gca()\n",
    "    plt.xticks(np.asarray(xlogr))\n",
    "    ax.set_xticklabels(param_grid['ridge'])\n",
    "    ax.set_title(\"Ridge\", fontsize=14)\n",
    "    plt.xlabel('λ',fontsize=14)\n",
    "    plt.ylabel('MSE',fontsize=14)\n",
    "    #plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(222)\n",
    "    # log x-axis\n",
    "    xlogl = np.log10(param_grid['lasso'])\n",
    "    plt.plot(xlogl, results[\"mse_test\"][\"lasso\"][0], label='p = %s' % p, linewidth=3.0) # plot ridge\n",
    "    ax = plt.gca()\n",
    "    plt.xticks(np.asarray(xlogl))\n",
    "    ax.set_xticklabels(param_grid['lasso'])\n",
    "    ax.set_title(\"Lasso\", fontsize=14)\n",
    "    plt.xlabel('λ', fontsize=14)\n",
    "    plt.ylabel('MSE', fontsize=14)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 0), loc='lower left', borderaxespad=0)\n",
    "\n",
    "\n",
    "     # One figure for R2 and all pol. orders\n",
    "    # Subplot for ridge\n",
    "    plt.subplot(223)\n",
    "    # log x-axis\n",
    "    xlogr = np.log10(param_grid['ridge'])\n",
    "    plt.plot(xlogr, results[\"r2_test\"][\"ridge\"][0], label='p = %s' % p, linewidth=3.0) # plot ridge\n",
    "    ax = plt.gca()\n",
    "    plt.xticks(np.asarray(xlogr))\n",
    "    ax.set_xticklabels(param_grid['ridge'])\n",
    "    #ax.set_title(\"Ridge\", fontsize=14)\n",
    "    plt.xlabel('λ', fontsize=14)\n",
    "    plt.ylabel('R2 score', fontsize=14)\n",
    "    #plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(224)\n",
    "    # log x-axis\n",
    "    xlogl = np.log10(param_grid['lasso'])\n",
    "    plt.plot(xlogl, results[\"r2_test\"][\"lasso\"][0], label='p = %s' % p, linewidth=3.0) # plot ridge\n",
    "    ax = plt.gca()\n",
    "    plt.xticks(np.asarray(xlogl))\n",
    "    ax.set_xticklabels(param_grid['lasso'])\n",
    "    #ax.set_title(\"Lasso\", fontsize=14)\n",
    "    plt.xlabel('λ', fontsize=14)\n",
    "    plt.ylabel('R2 score', fontsize=14)\n",
    "    \n",
    "    mse_ols.append(results[\"mse_test\"][\"ols\"][0])\n",
    "\n",
    "    \n",
    "# plt.savefig(\"results/figures/franke/MSEvsLambda/MSEvsLambda_seed105_zoomOut.png\")\n",
    "plt.tight_layout()    \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variation of error in terms of the polynomial degree for OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_ols = []\n",
    "for p in np.arange(1,11,1):\n",
    "    X = generateDesignmatrix(p,x,y)\n",
    "    \n",
    "    results, z_pred_best = model_comparison0(\n",
    "    models, param_grid, X, z, split_size=0.2\n",
    "    )\n",
    "    mse_ols.append(results[\"mse_test\"][\"ols\"][0])\n",
    "    \n",
    "    \n",
    "\n",
    "# Plot MSE for OLS\n",
    "p = np.arange(1,11,1)\n",
    "\n",
    "plt.figure(2, figsize = (7,7))\n",
    "# log x-axis\n",
    "plt.plot(p, mse_ols, linewidth=3.0) # plot ridge\n",
    "ax = plt.gca()\n",
    "plt.xticks(p)\n",
    "ax.set_title(\"OLS\", y=1.05, fontsize=17)\n",
    "plt.xlabel('p', fontsize=14)\n",
    "plt.ylabel('MSE', fontsize=14)\n",
    "plt.grid(True)\n",
    "#plt.savefig(\"results/figures/franke/MSEvsLambda/OLS_MSEvsLambda_seed105_p10.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test scores for p=5 as function of lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "param_grid = {\n",
    "    'ols': [0],\n",
    "    'ridge': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100, 1000],  \n",
    "    'lasso': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100, 1000]\n",
    "}\n",
    "\n",
    "p=5\n",
    "X = generateDesignmatrix(p,x,y)\n",
    "\n",
    "results, z_pred_best = model_comparison0(\n",
    "models, param_grid, X, z, split_size=0.2\n",
    ")\n",
    "\n",
    "# write loop instead of repeating code!!!\n",
    "\n",
    "# One figure for MSE and all pol. orders\n",
    "\n",
    "# Subplot for ridge\n",
    "plt.figure(1, figsize = (11,7))\n",
    "plt.subplot(1,2,1)\n",
    "xlogr = np.log10(param_grid['ridge'])  # log x-axis\n",
    "plt.plot(xlogr, (results[\"mse_train\"][\"ols\"]*np.ones(len(xlogr))).T, 'r', label='OLS (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, (results[\"mse_test\"][\"ols\"]*np.ones(len(xlogr))).T, 'r--', label='OLS (test)',  linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"mse_train\"][\"ridge\"]).T, 'b', label='Ridge (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"mse_test\"][\"ridge\"]).T, 'b--', label='Ridge (test)', linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"mse_train\"][\"lasso\"]).T, 'g', label='LASSO (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"mse_test\"][\"lasso\"]).T, 'g--', label='LASSO (test)', linewidth=3.0) \n",
    "ax = plt.gca()\n",
    "plt.xticks(np.asarray(xlogr))\n",
    "ax.set_xticklabels(param_grid['ridge'])\n",
    "#ax.set_title(ax.set_title(\"Performance of OLS, Ridge and LASSO regressions\"))\n",
    "plt.xlabel('λ', fontsize=14)\n",
    "plt.ylabel('MSE', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Subplot for ridge\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(xlogr, (results[\"r2_train\"][\"ols\"]*np.ones(len(xlogr))).T, 'r', label='OLS (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, (results[\"r2_test\"][\"ols\"]*np.ones(len(xlogr))).T, 'r--', label='OLS (test)', linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"r2_train\"][\"ridge\"]).T, 'b', label='Ridge (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"r2_test\"][\"ridge\"]).T, 'b--', label='Ridge (test)', linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"r2_train\"][\"lasso\"]).T, 'g', label='LASSO (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"r2_test\"][\"lasso\"]).T, 'g--', label='LASSO (test)', linewidth=3.0) \n",
    "ax = plt.gca()\n",
    "plt.xticks(np.asarray(xlogr))\n",
    "ax.set_xticklabels(param_grid['ridge'])\n",
    "#ax.set_title(ax.set_title(\"Performance of OLS, Ridge and LASSO\"))\n",
    "plt.xlabel('λ', fontsize=14)\n",
    "plt.ylabel('R2', fontsize=14)\n",
    "plt.legend()\n",
    "plt.suptitle(\"Performance of OLS, Ridge and LASSO regressions\", y=1.05, fontsize=17)\n",
    "\n",
    "#plt.savefig(\"results/figures/franke/performance/performance_p5.png\")\n",
    "plt.tight_layout()    \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"mse_train\"][\"ols\"]*np.ones(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results\n",
    "mse_ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample to estimate bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have selected the penalisation parameter tha gives the lowest MSE score, we can estimate the bias and the variance of the models by using the bootstrap resampling technique. We do this for the best 3 models, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental setup\n",
    "models = {\n",
    "    \"ols\": algorithms.OLS, \n",
    "}\n",
    "\n",
    "lmd = {\n",
    "    'ols': [0],\n",
    "}\n",
    "nboots = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test, z_pred_test, bias, var, beta, mse_test, mse_train, ci_beta = resample(models, lmd, X, z, nboots, split_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_beta "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We conclude from the first part that our best model is the OLS, therefore we continue the analysis only for that model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} >= {} + {} = {}'.format(mse_test['ols'], bias['ols'], var['ols'], bias['ols']+var['ols']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(1, figsize = (7,7))\n",
    "z_min = np.min([np.min(z_test['ols']), np.min(np.mean(z_pred_test['ols']))]) # minimum of all z_pred and z_pred_test\n",
    "z_max = np.max([np.max(z_test['ols']), np.max(np.mean(z_pred_test['ols']))])\n",
    "x_axis = [z_min, z_max]\n",
    "plt.scatter(z_test['ols'], np.mean(z_pred_test['ols'], axis=1))\n",
    "plt.plot(x_axis, x_axis, linewidth=3.0)\n",
    "plt.xlabel('original z', fontsize=14)\n",
    "plt.ylabel('predicted z', fontsize=14)\n",
    "plt.title('Compare prediction to original data for the OLS approach', y=1.05, fontsize=14)\n",
    "\n",
    "#plt.savefig(\"results/figures/franke/scatter_zpred/scatter_zpred_ols_p5\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to generate subplot for higher values of p\n",
    "\n",
    "bias_p = []\n",
    "var_p = []\n",
    "mse_test_p = []\n",
    "x = np.squeeze(x)\n",
    "y = np.squeeze(y)\n",
    "poly = np.arange(1,11,1)\n",
    "for p in poly:\n",
    "    X = generateDesignmatrix(p,x,y)\n",
    "    z_test, z_pred_test, bias, var, beta, mse_test, mse_train, ci_beta = resample(models, lmd, X, z, nboots, split_size = 0.2)\n",
    "    \n",
    "    bias_p.append(bias['ols'])\n",
    "    var_p.append(var['ols'])\n",
    "    mse_test_p.append(mse_test['ols'])\n",
    "    \n",
    "\n",
    "    \n",
    "plt.figure(1, figsize = (7,7))\n",
    "# plt.figure(1, figsize = (11,7))\n",
    "# plt.subplot(1,2,1)\n",
    "plt.plot(poly, mse_test_p, label='MSE', linewidth=3.0)\n",
    "plt.plot(poly, bias_p, label='Bias^2', linewidth=3.0)\n",
    "plt.plot(poly, var_p, label='Variance', linewidth=3.0)\n",
    "plt.xlabel('Polynomial degree', fontsize=14)\n",
    "plt.title('Bias-variance tradeoff', y=1.05, fontsize=17)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# bias_p = []\n",
    "# var_p = []\n",
    "# mse_test_p = []\n",
    "# poly = np.arange(11,26,1)\n",
    "# for p in poly:\n",
    "#     X = generateDesignmatrix(p,x,y)\n",
    "#     z_test, z_pred_test, bias, var, beta, mse_test, mse_train, ci_beta = resample(models, lmd, X, z, nboots, split_size = 0.2)\n",
    "#   \n",
    "#     bias_p.append(bias['ols'])\n",
    "#     var_p.append(var['ols'])\n",
    "#     mse_test_p.append(mse_test['ols'])\n",
    "    \n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.plot(poly, mse_test_p, label='MSE', linewidth=3.0)\n",
    "# plt.plot(poly, bias_p, label='Bias^2', linewidth=3.0)\n",
    "# plt.plot(poly, var_p, label='Variance', linewidth=3.0)\n",
    "# plt.xlabel('Polynomial degree', fontsize=14)\n",
    "# plt.title('Bias-variance tradeoff', y=1.05, fontsize=17)\n",
    "# plt.legend()\n",
    "\n",
    "#plt.savefig(\"results/figures/franke/bias_variance/bias_variance_ols_p10\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performance of the three approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# run resample for lamda 0.0001 to 100 and store the values in a dictionary\n",
    "\n",
    "X = generateDesignmatrix(5,np.squeeze(x),np.squeeze(y))\n",
    "\n",
    "models = {\n",
    "    \"ols\": algorithms.OLS, \n",
    "    'ridge': algorithms.Ridge, \n",
    "    \"lasso\": algorithms.Lasso\n",
    "}\n",
    "param_grid = {\n",
    "    'ols': [0],\n",
    "    'ridge': [0.0001],  \n",
    "    'lasso': [0.0001]\n",
    "}\n",
    "\n",
    "\n",
    "z_test, z_pred_test, bias, var, beta, mse_test, mse_train, ci_beta = resample(models, param_grid, X, z, nboots, split_size = 0.2) \n",
    "\n",
    "mse_test_lmd = {\n",
    "    'ols': [mse_test['ols']],\n",
    "    'ridge': [mse_test['ridge']],  \n",
    "    'lasso': [mse_test['lasso']]\n",
    "}\n",
    "\n",
    "mse_train_lmd = {\n",
    "    'ols': [mse_test['ols']],\n",
    "    'ridge': [mse_test['ridge']],  \n",
    "    'lasso': [mse_test['lasso']]\n",
    "}\n",
    "\n",
    "\n",
    "lmd_axis = param_grid['ridge']\n",
    "while param_grid['ridge'][0]<100:\n",
    "    z_test, z_pred_test, bias, var, beta, mse_test, mse_train, ci_beta = resample(models, param_grid, X, z, nboots, split_size = 0.2) \n",
    "    param_grid = dict((key, [x * 10 for x in values]) for key, values in param_grid.items())\n",
    "    lmd_axis.append(param_grid['ridge'][0])  # store lambda values for each iteration\n",
    "    for k in models:\n",
    "        print(k, mse_test[k])\n",
    "        mse_test_lmd[k].append(mse_test[k])\n",
    "        mse_train_lmd[k].append(mse_train[k])\n",
    "    \n",
    "\n",
    "# make the plot\n",
    "\n",
    "# Subplot for ridge\n",
    "plt.figure(1, figsize = (7,7))\n",
    "# log x-axis\n",
    "xlogr = np.log10(lmd_axis)\n",
    "plt.plot(xlogr, mse_train_lmd[\"ols\"], 'r', label='OLS (train)',linewidth=3.0) \n",
    "plt.plot(xlogr, mse_test_lmd[\"ols\"], 'r--', label='OLS (test)',linewidth=3.0) \n",
    "plt.plot(xlogr, mse_train_lmd[\"ridge\"], 'b', label='Ridge (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, mse_test_lmd[\"ridge\"], 'b--', label='Ridge (test)', linewidth=3.0) \n",
    "plt.plot(xlogr, mse_train_lmd[\"lasso\"], 'g', label='LASSO (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, mse_test_lmd[\"lasso\"], 'g--', label='LASSO (test)', linewidth=3.0) \n",
    "ax = plt.gca()\n",
    "plt.xticks(np.asarray(xlogr))\n",
    "ax.set_xticklabels(param_grid['ridge'])\n",
    "ax.set_title(\"Performance of OLS, Ridge and LASSO regressions\", y=1.05, fontsize=14)\n",
    "plt.xlabel('λ', fontsize=14)\n",
    "plt.ylabel('MSE', fontsize=14)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the code with SRTM data\n",
    "\n",
    "## We selected images from and Montevideo (Uruguay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the terrain\n",
    "terrain_montevideo = imread('Montevideo.tif')\n",
    "# Show the terrain\n",
    "plt.figure(1, figsize = (7,7))\n",
    "plt.title('Terrain over Montevideo', y=1.05, fontsize=17)\n",
    "plt.imshow(terrain_montevideo, cmap='gray')\n",
    "plt.xlabel('X', fontsize=14)\n",
    "plt.ylabel('Y', fontsize=14)\n",
    "plt.savefig(\"results/figures/montevideo/montevideo.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We subset the data, and generate a squared not singular design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthx = 200\n",
    "lengthy = 150\n",
    "terrain_montevideo = terrain_montevideo[0:lengthx,0:lengthy]\n",
    "terrain_montevideo.shape\n",
    "\n",
    "\n",
    "x = np.linspace(1, lengthx, num=lengthx)\n",
    "y = np.linspace(1, lengthy, num=lengthy)\n",
    "\n",
    "x = np.arange(lengthx)\n",
    "y = np.arange(lengthy)\n",
    "# Generate a grid\n",
    "x = np.squeeze(np.tile(x,[1,lengthy] ))\n",
    "y = np.squeeze(np.tile(y,[1,lengthx] )) \n",
    "z = np.ndarray.flatten(terrain_montevideo)\n",
    "\n",
    "p = 5\n",
    "X = generateDesignmatrix(p,x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental setup\n",
    "# Don't use alpha=0.001 because it can cause precision problems\n",
    "\n",
    "results_montevideo, z_pred_montevideo =  model_comparison0(\n",
    "    models, param_grid, X, z, split_size=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_montevideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow example in Piazza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from scipy.misc import imread\n",
    "from imageio import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "def surface_plot(surface, title, surface1=None, title1 = None):\n",
    "    M, N = surface.shape\n",
    "\n",
    "    ax_rows = np.arange(M)\n",
    "    ax_cols = np.arange(N)\n",
    "\n",
    "    [X, Y] = np.meshgrid(ax_cols, ax_rows)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    # Plot the predicted and the original surface\n",
    "    if surface1 is not None:\n",
    "        ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "        ax.plot_surface(X, Y, surface, cmap=cm.viridis, linewidth=0)\n",
    "        plt.title(title, fontsize=14)\n",
    "\n",
    "        ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "        ax.plot_surface(X, Y, surface1, cmap=cm.viridis, linewidth=0)\n",
    "        plt.title(title1, fontsize=14)\n",
    "    # Plot only the predicted surface\n",
    "    else:\n",
    "        ax = fig.gca(projection='3d')\n",
    "        ax.plot_surface(X, Y, surface, cmap=cm.viridis, linewidth=0)\n",
    "        plt.title(title, fontsize=14)\n",
    "\n",
    "\n",
    "def predict(rows, cols, beta):\n",
    "    out = np.zeros((np.size(rows), np.size(cols)))\n",
    "\n",
    "    for i, y_ in enumerate(rows):\n",
    "        for j, x_ in enumerate(cols):\n",
    "            data_vec = np.array([1, x_, y_, x_ ** 2, x_ * y_, y_ ** 2, \\\n",
    "                                 x_ ** 3, x_ ** 2 * y_, x_ * y_ ** 2, y_ ** 3, \\\n",
    "                                 x_ ** 4, x_ ** 3 * y_, x_ ** 2 * y_ ** 2, x_ * y_ ** 3, y_ ** 4, \\\n",
    "                                 x_ ** 5, x_ ** 4 * y_, x_ ** 3 * y_ ** 2, x_ ** 2 * y_ ** 3, x_ * y_ ** 4,\n",
    "                                 y_ ** 5])  # ,\\\n",
    "            #    x_**6, x_**5*y_, x_**4*y_**2, x_**3*y_**3,x_**2*y_**4, x_*y_**5, y_**6, \\\n",
    "            #    x_**7, x_**6*y_, x_**5*y_**2, x_**4*y_**3,x_**3*y_**4, x_**2*y_**5, x_*y_**6, y_**7, \\\n",
    "            #    x_**8, x_**7*y_, x_**6*y_**2, x_**5*y_**3,x_**4*y_**4, x_**3*y_**5, x_**2*y_**6, x_*y_**7,y_**8, \\\n",
    "            #    x_**9, x_**8*y_, x_**7*y_**2, x_**6*y_**3,x_**5*y_**4, x_**4*y_**5, x_**3*y_**6, x_**2*y_**7,x_*y_**8, y_**9])\n",
    "            out[i, j] = data_vec @ beta\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Load the terrain\n",
    "    terrain1 = imread('Montevideo.tif')\n",
    "    [n, m] = terrain1.shape\n",
    "\n",
    "    ## Find some random patches within the dataset and perform a fit\n",
    "\n",
    "    patch_size_row = 90\n",
    "    patch_size_col = 90\n",
    "    # patch_size_row = 50  # uncomment for smaller samples\n",
    "    # patch_size_col = 50  # patch_size_col = 50  \n",
    "\n",
    "\n",
    "    # Define their axes\n",
    "    rows = np.linspace(0, 1, patch_size_row)\n",
    "    cols = np.linspace(0, 1, patch_size_col)\n",
    "\n",
    "    [C, R] = np.meshgrid(cols, rows)\n",
    "\n",
    "    x = C.reshape(-1, 1)\n",
    "    y = R.reshape(-1, 1)\n",
    "\n",
    "    num_data = patch_size_row * patch_size_col\n",
    "\n",
    "    # Find the start indices of each patch\n",
    "\n",
    "    num_patches = 5\n",
    "\n",
    "    np.random.seed(41555)\n",
    "\n",
    "    row_starts = np.random.randint(0, n - patch_size_row, num_patches)\n",
    "    col_starts = np.random.randint(0, m - patch_size_col, num_patches)\n",
    "\n",
    "    for i, row_start, col_start in zip(np.arange(num_patches), row_starts, col_starts):\n",
    "        row_end = row_start + patch_size_row\n",
    "        col_end = col_start + patch_size_col\n",
    "\n",
    "        patch = terrain1[row_start:row_end, col_start:col_end]\n",
    "\n",
    "        z = patch.reshape(-1, 1)\n",
    "        \n",
    "        print('x', x.shape, 'y', y.shape, 'z', z.shape)\n",
    "\n",
    "        # Perform OLS fit\n",
    "        data = np.c_[np.ones((num_data, 1)), x, y, \\\n",
    "                     x ** 2, x * y, y ** 2, \\\n",
    "                     x ** 3, x ** 2 * y, x * y ** 2, y ** 3, \\\n",
    "                     x ** 4, x ** 3 * y, x ** 2 * y ** 2, x * y ** 3, y ** 4, \\\n",
    "                     x ** 5, x ** 4 * y, x ** 3 * y ** 2, x ** 2 * y ** 3, x * y ** 4, y ** 5]  # , \\\n",
    "        # x**6, x**5*y, x**4*y**2, x**3*y**3,x**2*y**4, x*y**5, y**6, \\\n",
    "        # x**7, x**6*y, x**5*y**2, x**4*y**3,x**3*y**4, x**2*y**5, x*y**6, y**7, \\\n",
    "        # x**8, x**7*y, x**6*y**2, x**5*y**3,x**4*y**4, x**3*y**5, x**2*y**6, x*y**7,y**8, \\\n",
    "        # x**9, x**8*y, x**7*y**2, x**6*y**3,x**5*y**4, x**4*y**5, x**3*y**6, x**2*y**7,x*y**8, y**9]\n",
    "\n",
    "        beta_ols = np.linalg.inv(data.T @ data) @ data.T @ z\n",
    "\n",
    "        fitted_patch = predict(rows, cols, beta_ols)\n",
    "\n",
    "        mse = np.sum((fitted_patch - patch) ** 2) / num_data\n",
    "        R2 = 1 - np.sum((fitted_patch - patch) ** 2) / np.sum((patch - np.mean(patch)) ** 2)\n",
    "        var = np.sum((fitted_patch - np.mean(fitted_patch)) ** 2) / num_data\n",
    "        bias = np.sum((patch - np.mean(fitted_patch)) ** 2) / num_data\n",
    "\n",
    "        print(\"patch %d, from (%d, %d) to (%d, %d)\" % (i + 1, row_start, col_start, row_end, col_end))\n",
    "        print(\"mse: %g\\nR2: %g\" % (mse, R2))\n",
    "        print(\"variance: %g\" % var)\n",
    "        print(\"bias: %g\\n\" % bias)\n",
    "\n",
    "        surface_plot(fitted_patch, 'Fitted terrain surface', patch, 'Real terrain surface')\n",
    "        \n",
    "        outfile = \"results/figures/montevideo/patch_mvd\" + str(i) + '.png'\n",
    "        #plt.savefig(outfile)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        p= 5\n",
    "        X = generateDesignmatrix(p,np.squeeze(x),np.squeeze(y))\n",
    "        \n",
    "        \n",
    "        results_montevideo, z_pred_montevideo =  model_comparison0(\n",
    "            models, param_grid, X, z, split_size=0.2\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "np.linalg.inv(X.T @ X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "print(z.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#beta_ols = np.linalg.inv(data.T @ data) @ data.T @ z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = generateDesignmatrix(p,np.squeeze(x),np.squeeze(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z_pred_montevideo['ridge'] .shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#fitted_patch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#50*60param_grid['ridge']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Experimental setup\n",
    "models = {\n",
    "    \"ols\": algorithms.OLS, \n",
    "}\n",
    "\n",
    "lmd = {\n",
    "    'ols': [0],\n",
    "}\n",
    "nboots = 100\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z_test, z_pred_test, bias, var, beta, mse_test, mse_train, ci_beta = resample(models, lmd, X, z, nboots, split_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat previous analysis for Montevideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test scores for p=5 as function of lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = {\n",
    "    \"ols\": algorithms.OLS, \n",
    "    'ridge': algorithms.Ridge, \n",
    "    \"lasso\": algorithms.Lasso\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'ols': [0],\n",
    "    'ridge': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100, 1000],  \n",
    "    'lasso': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100, 1000]\n",
    "}\n",
    "\n",
    "p=5\n",
    "X = generateDesignmatrix(p, np.squeeze(x), np.squeeze(y))\n",
    "\n",
    "results, z_pred_best = model_comparison0(\n",
    "models, param_grid, X, z, split_size=0.2\n",
    ")\n",
    "\n",
    "# write loop instead of repeating code!!!\n",
    "\n",
    "# One figure for MSE and all pol. orders\n",
    "\n",
    "# Subplot for ridge\n",
    "plt.figure(1, figsize = (11,7))\n",
    "plt.subplot(1,2,1)\n",
    "xlogr = np.log10(param_grid['ridge'])  # log x-axis\n",
    "plt.plot(xlogr, (results[\"mse_train\"][\"ols\"]*np.ones(len(xlogr))).T, 'r', label='OLS (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, (results[\"mse_test\"][\"ols\"]*np.ones(len(xlogr))).T, 'r--', label='OLS (test)',  linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"mse_train\"][\"ridge\"]).T, 'b', label='Ridge (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"mse_test\"][\"ridge\"]).T, 'b--', label='Ridge (test)', linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"mse_train\"][\"lasso\"]).T, 'g', label='LASSO (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"mse_test\"][\"lasso\"]).T, 'g--', label='LASSO (test)', linewidth=3.0) \n",
    "ax = plt.gca()\n",
    "plt.xticks(np.asarray(xlogr))\n",
    "ax.set_xticklabels(param_grid['ridge'])\n",
    "#ax.set_title(ax.set_title(\"Performance of OLS, Ridge and LASSO regressions\"))\n",
    "plt.xlabel('λ', fontsize=14)\n",
    "plt.ylabel('MSE', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Subplot for ridge\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(xlogr, (results[\"r2_train\"][\"ols\"]*np.ones(len(xlogr))).T, 'r', label='OLS (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, (results[\"r2_test\"][\"ols\"]*np.ones(len(xlogr))).T, 'r--', label='OLS (test)', linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"r2_train\"][\"ridge\"]).T, 'b', label='Ridge (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"r2_test\"][\"ridge\"]).T, 'b--', label='Ridge (test)', linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"r2_train\"][\"lasso\"]).T, 'g', label='LASSO (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"r2_test\"][\"lasso\"]).T, 'g--', label='LASSO (test)', linewidth=3.0) \n",
    "ax = plt.gca()\n",
    "plt.xticks(np.asarray(xlogr))\n",
    "ax.set_xticklabels(param_grid['ridge'])\n",
    "#ax.set_title(ax.set_title(\"Performance of OLS, Ridge and LASSO\"))\n",
    "plt.xlabel('λ', fontsize=14)\n",
    "plt.ylabel('R2', fontsize=14)\n",
    "plt.legend()\n",
    "plt.suptitle(\"Performance of OLS, Ridge and LASSO regressions\", y=1.05, fontsize=17)\n",
    "\n",
    "# plt.savefig(\"results/figures/montevideo/performance/performance_p5_mvd.png\")\n",
    "plt.tight_layout()    \n",
    "plt.show()\n",
    "#print(results[\"mse_train\"][\"ridge\"])\n",
    "#print(np.asarray(results[\"mse_train\"][\"ridge\"]).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only mse plot for ols and ridge\n",
    "\n",
    "plt.figure(1, figsize = (7,7))\n",
    "plt.plot(xlogr, (results[\"mse_train\"][\"ols\"]*np.ones(len(xlogr))).T, 'r', label='OLS (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, (results[\"mse_test\"][\"ols\"]*np.ones(len(xlogr))).T, 'r--', label='OLS (test)',  linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"mse_train\"][\"ridge\"]).T, 'b', label='Ridge (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"mse_test\"][\"ridge\"]).T, 'b--', label='Ridge (test)', linewidth=3.0) \n",
    "ax = plt.gca()\n",
    "plt.xticks(np.asarray(xlogr))\n",
    "ax.set_xticklabels(param_grid['ridge'])\n",
    "#ax.set_title(ax.set_title(\"Performance of OLS, Ridge and LASSO regressions\"))\n",
    "plt.xlabel('λ', fontsize=14)\n",
    "plt.ylabel('MSE', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"results/figures/montevideo/performance/performance_p5_mvd_olsRidge.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change split size\n",
    "\n",
    "results, z_pred_best = model_comparison0(\n",
    "models, param_grid, X, z, split_size=0.4\n",
    ")\n",
    "\n",
    "# only mse plot for ols and ridge\n",
    "\n",
    "plt.figure(1, figsize = (7,7))\n",
    "plt.plot(xlogr, (results[\"mse_train\"][\"ols\"]*np.ones(len(xlogr))).T, 'r', label='OLS (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, (results[\"mse_test\"][\"ols\"]*np.ones(len(xlogr))).T, 'r--', label='OLS (test)',  linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"mse_train\"][\"ridge\"]).T, 'b', label='Ridge (train)', linewidth=3.0) \n",
    "plt.plot(xlogr, np.asarray(results[\"mse_test\"][\"ridge\"]).T, 'b--', label='Ridge (test)', linewidth=3.0) \n",
    "ax = plt.gca()\n",
    "plt.xticks(np.asarray(xlogr))\n",
    "ax.set_xticklabels(param_grid['ridge'])\n",
    "#ax.set_title(ax.set_title(\"Performance of OLS, Ridge and LASSO regressions\"))\n",
    "plt.xlabel('λ', fontsize=14)\n",
    "plt.ylabel('MSE', fontsize=14)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"results/figures/montevideo/performance/performance_p5_mvd_olsRidge.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample to estimate bias and variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have selected the penalisation parameter tha gives the lowest MSE score, we can estimate the bias and the variance of the models by using the bootstrap resampling technique. We do this for the best 3 models, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental setup\n",
    "models = {\n",
    "    \"ols\": algorithms.OLS, \n",
    "}\n",
    "\n",
    "lmd = {\n",
    "    'ols': [0],\n",
    "}\n",
    "nboots = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test, z_pred_test, bias, var, beta, mse_test, mse_train, ci_beta = resample(models, lmd, X, z, nboots, split_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_beta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We conclude from the first part that our best model is the OLS, therefore we continue the analysis only for that model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} >= {} + {} = {}'.format(mse_test['ols'], bias['ols'], var['ols'], bias['ols']+var['ols']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(1, figsize = (7,7))\n",
    "z_min = np.min([np.min(z_test['ols']), np.min(np.mean(z_pred_test['ols']))]) # minimum of all z_pred and z_pred_test\n",
    "z_max = np.max([np.max(z_test['ols']), np.max(np.mean(z_pred_test['ols']))])\n",
    "x_axis = [z_min, z_max]\n",
    "plt.scatter(z_test['ols'], np.mean(z_pred_test['ols'], axis=1))\n",
    "plt.plot(x_axis, x_axis, linewidth=3.0)\n",
    "plt.xlabel('original z', fontsize=14)\n",
    "plt.ylabel('predicted z', fontsize=14)\n",
    "plt.title('Compare prediction to original data for the OLS approach', y=1.05, fontsize=14)\n",
    "\n",
    "# plt.savefig(\"results/figures/montevideo/scatter_zpred/scatter_zpred_ols_p5_mvd\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to generate subplot for higher values of p\n",
    "\n",
    "bias_p = []\n",
    "var_p = []\n",
    "mse_test_p = []\n",
    "x = np.squeeze(x)\n",
    "y = np.squeeze(y)\n",
    "poly = np.arange(1,12,1)\n",
    "for p in poly:\n",
    "    X = generateDesignmatrix(p,x,y)\n",
    "    z_test, z_pred_test, bias, var, beta, mse_test, mse_train, ci_beta = resample(models, lmd, X, z, nboots, split_size = 0.2)\n",
    "    \n",
    "    bias_p.append(bias['ols'])\n",
    "    var_p.append(var['ols'])\n",
    "    mse_test_p.append(mse_test['ols'])\n",
    "    \n",
    "\n",
    "    \n",
    "plt.figure(1, figsize = (7,7))\n",
    "# plt.figure(1, figsize = (12,7))\n",
    "# plt.subplot(1,2,1)\n",
    "plt.plot(poly, mse_test_p, label='MSE', linewidth=3.0)\n",
    "plt.plot(poly, bias_p, label='Bias^2', linewidth=3.0)\n",
    "plt.plot(poly, var_p, label='Variance', linewidth=3.0)\n",
    "plt.xlabel('Polynomial degree', fontsize=14)\n",
    "plt.title('Bias-variance tradeoff', y=1.05, fontsize=17)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# bias_p = []\n",
    "# var_p = []\n",
    "# mse_test_p = []\n",
    "# poly = np.arange(11,26,1)\n",
    "# for p in poly:\n",
    "#     X = generateDesignmatrix(p,x,y)\n",
    "#     z_test, z_pred_test, bias, var, beta, mse_test, mse_train, ci_beta = resample(models, lmd, X, z, nboots, split_size = 0.2)\n",
    "#   \n",
    "#     bias_p.append(bias['ols'])\n",
    "#     var_p.append(var['ols'])\n",
    "#     mse_test_p.append(mse_test['ols'])\n",
    "    \n",
    "\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.plot(poly, mse_test_p, label='MSE', linewidth=3.0)\n",
    "# plt.plot(poly, bias_p, label='Bias^2', linewidth=3.0)\n",
    "# plt.plot(poly, var_p, label='Variance', linewidth=3.0)\n",
    "# plt.xlabel('Polynomial degree', fontsize=14)\n",
    "# plt.title('Bias-variance tradeoff', y=1.05, fontsize=17)\n",
    "# plt.legend()\n",
    "\n",
    "#plt.savefig(\"results/figures/montevideo/bias_variance/bias_variance_ols_p11_mvd\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental setup\n",
    "\n",
    "## Define the models and the parameters that we want to compare.\n",
    "The model are Ordinary Least Squares (OLS), Ridge and Lasso.\n",
    "The parameter are []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"ols\": algorithms.OLS, \n",
    "    'ridge': algorithms.Ridge, \n",
    "    \"lasso\": algorithms.Lasso\n",
    "}\n",
    "param_grid = {\n",
    "    'ols': [0],\n",
    "    'ridge': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100, 1000],  \n",
    "    'lasso': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100, 1000]\n",
    "}\n",
    "\n",
    "# for lower values of parameters, uncomment\n",
    "\n",
    "# param_grid = {\n",
    "#     'ols': [0],\n",
    "#     'ridge': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0],  \n",
    "#     'lasso': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0]\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform experiment and collect results.\n",
    " \n",
    "The sample is splitted leaving 80% of the data for training the model and 20% for testing it.\n",
    "\n",
    "The polynomial order varies from 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_ols = []\n",
    "for p in np.arange(1,6,1):\n",
    "    X = generateDesignmatrix(p, np.squeeze(x), np.squeeze(y))\n",
    "    \n",
    "    results, z_pred_best = model_comparison0(\n",
    "    models, param_grid, X, z, split_size=0.2\n",
    "    )\n",
    "\n",
    "    # write loop instead of repeating code!!!\n",
    "    \n",
    "    # One figure for MSE and all pol. orders\n",
    "\n",
    "    # Subplot for ridge\n",
    "    plt.figure(1, figsize = (11,7))\n",
    "    plt.subplot(221)\n",
    "    # log x-axis\n",
    "    xlogr = np.log10(param_grid['ridge'])\n",
    "    plt.plot(xlogr, results[\"mse_test\"][\"ridge\"][0], label='p = %s' % p, linewidth=3.0) # plot ridge\n",
    "    ax = plt.gca()\n",
    "    plt.xticks(np.asarray(xlogr))\n",
    "    ax.set_xticklabels(param_grid['ridge'])\n",
    "    ax.set_title(\"Ridge\", fontsize=14)\n",
    "    plt.xlabel('λ',fontsize=14)\n",
    "    plt.ylabel('MSE',fontsize=14)\n",
    "    #plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(222)\n",
    "    # log x-axis\n",
    "    xlogl = np.log10(param_grid['lasso'])\n",
    "    plt.plot(xlogl, results[\"mse_test\"][\"lasso\"][0], label='p = %s' % p, linewidth=3.0) # plot ridge\n",
    "    ax = plt.gca()\n",
    "    plt.xticks(np.asarray(xlogl))\n",
    "    ax.set_xticklabels(param_grid['lasso'])\n",
    "    ax.set_title(\"Lasso\", fontsize=14)\n",
    "    plt.xlabel('λ', fontsize=14)\n",
    "    plt.ylabel('MSE', fontsize=14)\n",
    "    ax.legend(bbox_to_anchor=(1.05, 0), loc='lower left', borderaxespad=0)\n",
    "\n",
    "\n",
    "     # One figure for R2 and all pol. orders\n",
    "    # Subplot for ridge\n",
    "    plt.subplot(223)\n",
    "    # log x-axis\n",
    "    xlogr = np.log10(param_grid['ridge'])\n",
    "    plt.plot(xlogr, results[\"r2_test\"][\"ridge\"][0], label='p = %s' % p, linewidth=3.0) # plot ridge\n",
    "    ax = plt.gca()\n",
    "    plt.xticks(np.asarray(xlogr))\n",
    "    ax.set_xticklabels(param_grid['ridge'])\n",
    "    #ax.set_title(\"Ridge\", fontsize=14)\n",
    "    plt.xlabel('λ', fontsize=14)\n",
    "    plt.ylabel('R2 score', fontsize=14)\n",
    "    #plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "    plt.subplot(224)\n",
    "    # log x-axis\n",
    "    xlogl = np.log10(param_grid['lasso'])\n",
    "    plt.plot(xlogl, results[\"r2_test\"][\"lasso\"][0], label='p = %s' % p, linewidth=3.0) # plot ridge\n",
    "    ax = plt.gca()\n",
    "    plt.xticks(np.asarray(xlogl))\n",
    "    ax.set_xticklabels(param_grid['lasso'])\n",
    "    #ax.set_title(\"Lasso\", fontsize=14)\n",
    "    plt.xlabel('λ', fontsize=14)\n",
    "    plt.ylabel('R2 score', fontsize=14)\n",
    "    \n",
    "    mse_ols.append(results[\"mse_test\"][\"ols\"][0])\n",
    "\n",
    "    \n",
    "# plt.savefig(\"results/figures/montevideo/MSEvsLambda/MSEvsLambda_seed105_zoomOut_mvd.png\")\n",
    "plt.tight_layout()    \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too long to run\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for i, row_start, col_start in zip(np.arange(num_patches), row_starts, col_starts):\n",
    "    row_end = row_start + patch_size_row\n",
    "    col_end = col_start + patch_size_col\n",
    "\n",
    "    patch = terrain1[row_start:row_end, col_start:col_end]\n",
    "\n",
    "    z = patch.reshape(-1, 1)\n",
    "    \n",
    "    mse_ols = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    for p in np.arange(1,11,1):\n",
    "        X = generateDesignmatrix(p, np.squeeze(x), np.squeeze(y))\n",
    "\n",
    "        results, z_pred_best = model_comparison0(\n",
    "        models, param_grid, X, z, split_size=0.2\n",
    "        )\n",
    "        mse_ols.append(results[\"mse_test\"][\"ols\"][0])\n",
    "\n",
    "\n",
    "\n",
    "    # Plot MSE for OLS\n",
    "    p = np.arange(1,11,1)\n",
    "\n",
    "    plt.figure(1, figsize = (7,7))\n",
    "    plt.subplot()\n",
    "    # log x-axis\n",
    "    plt.plot(p, mse_ols, linewidth=3.0, label='Patch Nr. %s' % i) # plot ridge\n",
    "    ax = plt.gca()\n",
    "    plt.xticks(p)\n",
    "    ax.set_title(\"OLS\", y=1.05, fontsize=17)\n",
    "    plt.xlabel('p', fontsize=14)\n",
    "    plt.ylabel('MSE', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "plt.savefig(\"results/figures/montevideo/MSEvsLambda/OLS_MSEvsLambda_seed105_p10_mvd.png\")\n",
    "plt.show()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variation of error in terms of the polynomial degree for OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_ols = []\n",
    "for p in np.arange(1,11,1):\n",
    "    X = generateDesignmatrix(p, np.squeeze(x), np.squeeze(y))\n",
    "    \n",
    "    results, z_pred_best = model_comparison0(\n",
    "    models, param_grid, X, z, split_size=0.2\n",
    "    )\n",
    "    mse_ols.append(results[\"mse_test\"][\"ols\"][0])\n",
    "    \n",
    "    \n",
    "\n",
    "# Plot MSE for OLS\n",
    "p = np.arange(1,11,1)\n",
    "\n",
    "plt.figure(2, figsize = (7,7))\n",
    "# log x-axis\n",
    "plt.plot(p, mse_ols, linewidth=3.0) # plot ridge\n",
    "ax = plt.gca()\n",
    "plt.xticks(p)\n",
    "ax.set_title(\"OLS\", y=1.05, fontsize=17)\n",
    "plt.xlabel('p', fontsize=14)\n",
    "plt.ylabel('MSE', fontsize=14)\n",
    "plt.grid(True)\n",
    "#plt.savefig(\"results/figures/montevideo/MSEvsLambda/OLS_MSEvsLambda_seed105_p10_mvd_patch4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
